{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.4.4"
    },
    "colab": {
      "name": "bootstrap.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiNOV-Tokyo/yolov5/blob/main/bootstrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTEWmb0gqQW6",
        "outputId": "da103dbd-313c-45d2-c59a-a8402706a7f9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od0bN3aqqU5w",
        "outputId": "8d8c61da-088e-4535-e17d-a771689f4085"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Bootstrap"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Bootstrap\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQymhXEUqN88"
      },
      "source": [
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential, model_from_json\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#from keras.utils import to_categorical\n",
        "from keras.layers.core import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "#from keras.optimizers import RMSprop\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, concatenate , Input, Reshape, Dense\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "import numpy as np\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2WKIYQpqN9G"
      },
      "source": [
        "dir_name = 'resources/eval_light/'\n",
        "\n",
        "# Read a file and return a string\n",
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "def load_data(data_dir):\n",
        "    text = []\n",
        "    images = []\n",
        "    # Load all the files and order them\n",
        "    all_filenames = listdir(data_dir)\n",
        "    all_filenames.sort()\n",
        "    for filename in (all_filenames):\n",
        "        if filename[-3:] == \"npz\":\n",
        "            # Load the images already prepared in arrays\n",
        "            image = np.load(data_dir+filename)\n",
        "            images.append(image['features'])\n",
        "        else:\n",
        "            # Load the boostrap tokens and rap them in a start and end tag\n",
        "            syntax = '<START> ' + load_doc(data_dir+filename) + ' <END>'\n",
        "            # Seperate all the words with a single space\n",
        "            syntax = ' '.join(syntax.split())\n",
        "            # Add a space after each comma\n",
        "            syntax = syntax.replace(',', ' ,')\n",
        "            text.append(syntax)\n",
        "    images = np.array(images, dtype=float)\n",
        "    return images, text\n",
        "\n",
        "train_features, texts = load_data(dir_name)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID4zm2GhqN9I"
      },
      "source": [
        "# Initialize the function to create the vocabulary \n",
        "tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
        "# Create the vocabulary \n",
        "tokenizer.fit_on_texts([load_doc('resources/bootstrap.vocab')])\n",
        "\n",
        "# Add one spot for the empty word in the vocabulary \n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "# Map the input sentences into the vocabulary indexes\n",
        "train_sequences = tokenizer.texts_to_sequences(texts)\n",
        "# The longest set of boostrap tokens\n",
        "max_sequence = max(len(s) for s in train_sequences)\n",
        "# Specify how many tokens to have in each input sentence\n",
        "max_length = 48\n",
        "\n",
        "def preprocess_data(sequences, features):\n",
        "    X, y, image_data = list(), list(), list()\n",
        "    for img_no, seq in enumerate(sequences):\n",
        "        for i in range(1, len(seq)):\n",
        "            # Add the sentence until the current count(i) and add the current count to the output\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            # Pad all the input token sentences to max_sequence\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_sequence)[0]\n",
        "            # Turn the output into one-hot encoding\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            # Add the corresponding image to the boostrap token file\n",
        "            image_data.append(features[img_no])\n",
        "            # Cap the input sentence to 48 tokens and add it\n",
        "            X.append(in_seq[-48:])\n",
        "            y.append(out_seq)\n",
        "    return np.array(X), np.array(y), np.array(image_data)\n",
        "\n",
        "X, y, image_data = preprocess_data(train_sequences, train_features)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM7HknT5qN9K",
        "outputId": "7228ede1-4824-44f9-b9bd-eb09271f8683"
      },
      "source": [
        "#Create the encoder\n",
        "image_model = Sequential()\n",
        "image_model.add(Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(256, 256, 3,)))\n",
        "image_model.add(Conv2D(16, (3,3), activation='relu', padding='same', strides=2))\n",
        "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
        "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\n",
        "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
        "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\n",
        "image_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
        "\n",
        "image_model.add(Flatten())\n",
        "image_model.add(Dense(1024, activation='relu'))\n",
        "image_model.add(Dropout(0.3))\n",
        "image_model.add(Dense(1024, activation='relu'))\n",
        "image_model.add(Dropout(0.3))\n",
        "\n",
        "image_model.add(RepeatVector(max_length))\n",
        "\n",
        "visual_input = Input(shape=(256, 256, 3,))\n",
        "encoded_image = image_model(visual_input)\n",
        "\n",
        "language_input = Input(shape=(max_length,))\n",
        "language_model = Embedding(vocab_size, 50, input_length=max_length, mask_zero=True)(language_input)\n",
        "language_model = LSTM(128, return_sequences=True)(language_model)\n",
        "language_model = LSTM(128, return_sequences=True)(language_model)\n",
        "\n",
        "#Create the decoder\n",
        "decoder = concatenate([encoded_image, language_model])\n",
        "decoder = LSTM(512, return_sequences=True)(decoder)\n",
        "decoder = LSTM(512, return_sequences=False)(decoder)\n",
        "decoder = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[visual_input, language_input], outputs=decoder)\n",
        "optimizer = RMSprop(lr=0.0001, clipvalue=1.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox9O_rzGqN9L",
        "outputId": "bd1c3a79-0af4-4c81-c911-63888e65a40d"
      },
      "source": [
        "#Save the model for every 2nd epoch\n",
        "filepath=\"org-weights-epoch-{epoch:04d}--val_loss-{val_loss:.4f}--loss-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=True, period=2)\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsbJVKNxqN9L",
        "outputId": "756e69b4-0736-498f-dc93-a1921bf1e3bb"
      },
      "source": [
        "# Train the model\n",
        "model.fit([image_data, X], y, batch_size=1, shuffle=False, validation_split=0.1, callbacks=callbacks_list, verbose=1, epochs=3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "624/624 [==============================] - 1173s 2s/step - loss: 2.4260 - val_loss: 2.3404\n",
            "Epoch 2/3\n",
            "624/624 [==============================] - ETA: 0s - loss: 2.3357\n",
            "Epoch 00002: saving model to org-weights-epoch-0002--val_loss-2.4638--loss-2.3357.hdf5\n",
            "624/624 [==============================] - 1173s 2s/step - loss: 2.3357 - val_loss: 2.4638\n",
            "Epoch 3/3\n",
            "624/624 [==============================] - 1178s 2s/step - loss: 1.9624 - val_loss: 2.2513\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff88287edd0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mx4KdqyqN9N"
      },
      "source": [
        "\n",
        "\n",
        "from keras.models import load_model\n",
        "model.save('bootstrap_model.h5')\n",
        "#model = load_model('html_model.h5')\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DxTw17g7-4z"
      },
      "source": [
        "\n",
        "\n",
        "from keras.models import load_model\n",
        "model.save('bootstrap_model.h5')\n",
        "#model = load_model('html_model.h5')\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2dohoQQAzf-"
      },
      "source": [
        "\n",
        "\n",
        "model.save_weights('bootstrap_weight.hdf5')\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgkfpu4d-sEH",
        "outputId": "397d0bef-55ff-495d-ed92-b150def696a1"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n",
            "219070464/219055592 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "o8svPVIt8CxF",
        "outputId": "d7dcda53-7c35-4a14-f9a6-41550355ab94"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7f51af219ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIR2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(test_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-2ccde9814f9a>\u001b[0m in \u001b[0;36mgenerate_desc\u001b[0;34m(model, tokenizer, photo, max_length)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# predict next word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphoto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# convert probability to integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 256, 256, 3), found shape=(None, 8, 8, 1536)\n"
          ]
        }
      ]
    }
  ]
}