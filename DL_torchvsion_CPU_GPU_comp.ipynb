{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_torchvsion_CPU_GPU_comp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2HpH0qGDHjENeMRO2KUcP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiNOV-Tokyo/yolov5/blob/main/DL_torchvsion_CPU_GPU_comp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKZWAeRLlM5b"
      },
      "source": [
        "# PyTorch実践入門\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2B6jhvTt-9I"
      },
      "source": [
        "## Pytorchにおけるテンソルの扱い"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9zG3qfX5prR"
      },
      "source": [
        "## 1.1 Google Drive をマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7Y7OSEWgYze",
        "outputId": "374b70de-a028-4cab-ea66-1ad8f30cfcf7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57-xQXjlghOq",
        "outputId": "a386844b-d688-4b16-f334-75e18fc10ed6"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks\n",
        "#%mkdir yolo_train\n",
        "%cd yolo_train"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n",
            "/content/drive/My Drive/Colab Notebooks/yolo_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORQPyRiV5xCd"
      },
      "source": [
        "## 1.2 モジュール読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59KLpqqeXs9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d313a66-7e39-498c-932c-7e4f1b49d43c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import os\n",
        "import copy\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n",
        "\n",
        "# GPU/CPUが使えるかどうか確認\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.9.0+cu102\n",
            "Torchvision Version:  0.10.0+cu102\n",
            "Setup complete. Using torch 1.9.0+cu102 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15109MB, multi_processor_count=40)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8fWcSwaDkoy"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "#dir(optim)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLBoFXvBkHnY"
      },
      "source": [
        "# 7. 画像分類モデルの構築"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf_JCSaPn05X"
      },
      "source": [
        "### 画像データセット"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b25shI_Tn9-0"
      },
      "source": [
        "from torchvision import datasets\n",
        "data_path = \"./CIFAR10\"\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=False)\n",
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqQnDDCbgxei"
      },
      "source": [
        "データの正規化のため、mean, stdを求める。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGY-s_vhg4o_",
        "outputId": "ee79575f-97a7-4c40-8ba6-af89216358f1"
      },
      "source": [
        "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform = transforms.ToTensor())\n",
        "tensor_cifar10_val = datasets.CIFAR10(data_path, train=False, download=False, transform = transforms.ToTensor())\n",
        "\n",
        "imgs_train = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
        "imgs_train.shape\n",
        "\n",
        "imgs_val = torch.stack([img_v for img_v, _ in tensor_cifar10_val], dim=3)\n",
        "imgs_val.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MsrCrSciZq1",
        "outputId": "3b4fb949-ae69-4e45-98c3-bb9aea51ffed"
      },
      "source": [
        "a1 = imgs_train.view(3, -1).mean(dim=1)\n",
        "a2 = imgs_train.view(3, -1).std(dim=1)\n",
        "\n",
        "b1 = imgs_val.view(3, -1).mean(dim=1)\n",
        "b2 = imgs_val.view(3, -1).std(dim=1)\n",
        "\n",
        "a1, a2, b1, b2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.4914, 0.4822, 0.4465]),\n",
              " tensor([0.2470, 0.2435, 0.2616]),\n",
              " tensor([0.4942, 0.4851, 0.4504]),\n",
              " tensor([0.2467, 0.2429, 0.2616]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVaemPtho8LC"
      },
      "source": [
        "Datasetの変換"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zV76Ri_rISQ"
      },
      "source": [
        "算出したmean, stdで再度データを読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKIIl1UYo-bi"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "transformed_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
        "                                       transform=transforms.Compose([\n",
        "                                                                     transforms.ToTensor(),\n",
        "                                                                     transforms.Normalize(a1,a2)\n",
        "                                       ]))\n",
        "transformed_cifar10_val= datasets.CIFAR10(data_path, train=False, download=False,\n",
        "                                       transform=transforms.Compose([\n",
        "                                                                     transforms.ToTensor(),\n",
        "                                                                     transforms.Normalize(b1,b2)\n",
        "                                       ]))\n",
        "\n",
        "#transformed_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
        " #                                      transform=transforms.Compose([\n",
        " #                                                                    transforms.ToTensor(),\n",
        " #                                                                    transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        " #                                                                                         (0.2470, 0.2435, 0.2616))\n",
        " #                                      ]))\n",
        "#transformed_cifar10_val= datasets.CIFAR10(data_path, train=False, download=False,\n",
        " #                                      transform=transforms.Compose([\n",
        "  #                                                                   transforms.ToTensor(),\n",
        "  #                                                                   transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "  #                                                                                        (0.2470, 0.2435, 0.2616))\n",
        "  #                                     ]))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7IRnqDhrAUJ"
      },
      "source": [
        "全結合モデル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNd4SVHZrCZj"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "n_out= 2\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(\n",
        "        3072,  #入力32 x 32 x 3\n",
        "        512,  # 隠れ層のサイズ\n",
        "    ),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(\n",
        "        512,\n",
        "        n_out,\n",
        "    ),\n",
        "    nn.Softmax(dim=1)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJq7_Ghzp98X"
      },
      "source": [
        "label_map = {0: 0, 2: 1}\n",
        "class_names = ['airplane', 'bird']\n",
        "cifar2 = [(img, label_map[label]) for img, label in transformed_cifar10 if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label]) for img, label in transformed_cifar10_val if label in [0, 2]]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Z4LYh1bXlM"
      },
      "source": [
        "## 8. 畳み込み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvWAPEdsqEDy"
      },
      "source": [
        "## 8.3 nn.Moduleを継承したモデル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgIHK-pFqBl9"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8*8*8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8*8*8)\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxiHUVkOqAFj",
        "outputId": "a266ba1c-9597-4fd1-b7d6-99dee9a0080d"
      },
      "source": [
        "model = Net()\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvPGUm7teVP"
      },
      "source": [
        "### 8.3.3 functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBQlJ60CtkUF"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(8*8*8, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8*8*8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuntT9RRvCB-"
      },
      "source": [
        "## 8.3 畳み込みニューラルネットワークの学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV_83Ya_T9hS"
      },
      "source": [
        "### 8.4.3 GPUGPU上での訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t0CjgvLUCgA"
      },
      "source": [
        "# GPU/CPUが使えるかどうか確認\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device('cpu');"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TubKxe5UUCy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7xY45nEUU7Z"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  \n",
        "    for epoch in range(1, n_epochs + 1 ):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device = device) #データをGPUGPUに移動\n",
        "            labels = labels.to(device = device) #データをGPUGPUに移動\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "        \n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "\n",
        "            print(\"{} Epoch {}: Training loss {}\".format(datetime.datetime.now(), epoch, loss_train / len(train_loader)))\n",
        "          "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir17uPvHVUsX",
        "outputId": "9b46c49a-451a-4d84-b500-06caf4ee18c8"
      },
      "source": [
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to(device=device)\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-08-31 03:58:06.611985 Epoch 1: Training loss 0.5938093070011989\n",
            "2021-08-31 03:58:09.454845 Epoch 10: Training loss 0.3229879477790966\n",
            "2021-08-31 03:58:12.580774 Epoch 20: Training loss 0.29016538847024276\n",
            "2021-08-31 03:58:15.720904 Epoch 30: Training loss 0.2731605195885251\n",
            "2021-08-31 03:58:18.814375 Epoch 40: Training loss 0.2557778714853487\n",
            "2021-08-31 03:58:21.939328 Epoch 50: Training loss 0.24080736510408152\n",
            "2021-08-31 03:58:25.042580 Epoch 60: Training loss 0.22631315303266428\n",
            "2021-08-31 03:58:28.115095 Epoch 70: Training loss 0.21274486027515618\n",
            "2021-08-31 03:58:31.202206 Epoch 80: Training loss 0.20048196091773404\n",
            "2021-08-31 03:58:34.350366 Epoch 90: Training loss 0.18483999101029838\n",
            "2021-08-31 03:58:37.536147 Epoch 100: Training loss 0.1699893241095695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcCSDAXZhpmC",
        "outputId": "d54829ef-1f43-46cd-be1b-e1a04c1c8858"
      },
      "source": [
        "# モデル\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "label_map = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'bird']\n",
        "cifar5 = [(img, label_map[label]) for img, label in transformed_cifar10 if label in [0, 1, 2, 3, 4]]\n",
        "cifar5_val = [(img, label_map[label]) for img, label in transformed_cifar10_val if label in [0, 1, 2, 3, 4]]\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar5, batch_size=64, shuffle=True)\n",
        "\n",
        "n_out= 5\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear( 3072, 512 ),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear( 512, n_out),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(8*8*8, 32)\n",
        "        self.fc2 = nn.Linear(32, n_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8*8*8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  \n",
        "    for epoch in range(1, n_epochs + 1 ):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device = device) #データをGPUGPUに移動\n",
        "            labels = labels.to(device = device) #データをGPUGPUに移動\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "        \n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "\n",
        "            print(\"{} Epoch {}: Training loss {}\".format(datetime.datetime.now(), epoch, loss_train / len(train_loader)))\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device('cpu')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar5, batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to(device=device)\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "#    n_epochs = 30,\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-08-31 04:00:33.907591 Epoch 1: Training loss 1.4480056076708352\n",
            "2021-08-31 04:00:40.847248 Epoch 10: Training loss 0.8388900881838006\n",
            "2021-08-31 04:00:48.603726 Epoch 20: Training loss 0.7079004489857218\n",
            "2021-08-31 04:00:56.399176 Epoch 30: Training loss 0.6245004819787067\n",
            "2021-08-31 04:01:04.098910 Epoch 40: Training loss 0.5718211863961671\n",
            "2021-08-31 04:01:11.830791 Epoch 50: Training loss 0.5268248888232824\n",
            "2021-08-31 04:01:19.538137 Epoch 60: Training loss 0.4888576419304704\n",
            "2021-08-31 04:01:27.305802 Epoch 70: Training loss 0.455706363710601\n",
            "2021-08-31 04:01:35.027315 Epoch 80: Training loss 0.42358576573069445\n",
            "2021-08-31 04:01:42.843101 Epoch 90: Training loss 0.3949828129595198\n",
            "2021-08-31 04:01:50.534363 Epoch 100: Training loss 0.3684179690259192\n"
          ]
        }
      ]
    }
  ]
}